{"title":"A (less) technical guide for understanding large language models","path":"/understanding-large-language-models/","site":"https://misaligned.markets","atUri":"at://did:plc:eg4afougfvkc6eofrjgmnia5/site.standard.document/3mf4vf7k7bnw2","publishedAt":"2025-11-14T21:15:19Z","tags":"is here;2]2 for bigger language models and other types of GenAI. The intensity of this boom and its relatively narrow focus are why I think it’s important to distinguish LLMs and generative AI [from other types of machine learning and AI technologies;_pre-training process_;_tens of terabytes of text_;4]4 While it’s not inaccurate to frame this as feeding the model the entire internet, given that foundation model companies will grab anything that’s on the open web (to their own [ _detriment_;_context window_;process tokens;Sora 2;for language models;_embedded all of its token-level associations_;_high-dimensional vector space_;_https://www.youtube.com/shorts/FJtFZwbvkI4_;_biased away from non-Western cultures_;_US dominated internet_;_audio models_;_produce music_;_Glaze_;_Nightshade_;_not work_;_only a small number of “bad” samples_;_models trained on these sources_;_a mathematical impossibility_;_stochastic parrots_;anterograde amnesia;10]10 See deeper patterns within language. Basically, we can think of LLMs as the protagonist from Christopher Nolan’s [ _Memento_;_subtle distinction_;_without knowing how to count the r’s in strawberry_;the static associations;_prompt engineer_;_lossy compressions_;_technical_;_tweaks_;_is somewhat vague_;_models trained on the game Othello_;__Language Models Represent Space and Time__;_constrained and deterministic_;_predominantly be represented in language_;_Inconsistency of LLMs in Molecular Representations_;_What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models_;_relied on surface-level patterns_;_Large Language Models and Mathematical Reasoning Failures_;_Syntactic Blind Spots: How Misalignment Leads to LLMs’ Mathematical Errors_;_Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning_;_some researchers_;_beginning_;_say_;_95% of AI pilots fail_;intelligence;_System prompts_;_leaked older version_;_introduce mistakes_;_Guardrails_;_renowned veteran_;_program databases_;_adversarial attacks_;engage in a state-sanctioned hacking campaign;JSON;_played Pokémon Red_;_fine-tuned_;Retrieval Augmented Generation;adding metadata to content;Knowledge graphs;track relationships between entities;build a graph-based system;Emerging RAG frameworks;context engineering;highlighting the limitations of the standard;security considerations;privileged user;_shaped my understanding_;_information asymmetry functions within our modern economy_;https://youtu.be/wjZofJX0v4M?si=XdqdoBWdAPAZLFL2;back in 1983;gradient descent;on to counting b's in blueberry;“You shall know a word by the company it keeps.”;Fine-tuning;Simplified Molecular Input Line Entry System;newer Claude prompt"}